{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of Adaboost Algorithm\n",
    "In this part we implement adaboost with ipython notebook. First let's load and play with the data. The data set contains two classes. It cannot be separated with a linear classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy  import *\n",
    "url = 'https://coding.net/u/HongHuangNeu/p/Machine-Learning-Notes-Data/git/raw/master/AdaBoost/data.txt'\n",
    "df = pd.read_csv(url,sep='\\t')\n",
    "df[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cmap = {1: 'red', -1: 'blue'}\n",
    "df.plot(x='x1', y='x2', kind='scatter', c=[cmap.get(c, 'black') for c in df.label])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaboost Algorithm\n",
    "\n",
    "Assume a 2-class classification problem. Training set is $\\{(x_1,y_1),(x_2,y_2),...(x_N,y_N)\\}$, where $y \\in \\{-1,+1\\}$. \n",
    "\n",
    "1. Initially, all training points have the same weight: $w_{i}^{(0)}=\\frac{1}{N}$. Choose a weak classification model $\\phi(x;\\theta)\\in \\{-1,1\\}$.\n",
    "\n",
    "2. Repeat the following for each iteration $m$: train a weak classifier $\\phi(x;\\theta_{m})$ on the training set such that the weighted error\n",
    "    $$P_{m}=\\sum^{N}_{i=1} w_{i}^{(m)} I(1-y_{i}\\phi(x_{i};\\theta_{m}))$$\n",
    "\n",
    "    is minimized\n",
    "\n",
    "    where $I(x)$ is 0 when $x$ is 0 and 1 when $x$ is non-zero\n",
    "\n",
    "    The weight for base classifier $\\phi(x;\\theta_{m})$ is\n",
    "    $$\\alpha_{m}=\\frac{1}{2}\\ln \\frac{1-P_{m}}{P_{m}}$$\n",
    "\n",
    "    The weight of each data point for the next iteration\n",
    "\n",
    "    $$w_{i}^{(m+1)}=\\frac{w_{i}^{(m)}exp(-y_{i}\\alpha_{m}\\phi(x_{i};\\theta_{m}))}{z_{m}} $$\n",
    "    where $z_{m}$ is the normalized factor.\n",
    "\n",
    "3. The resulting classifier is \n",
    "    $$f(x)=sign\\{F(x)\\}$$\n",
    "    where \n",
    "    $$F(x)=\\sum_{k=1}^{K}\\alpha_{k}\\phi(x;\\theta_{k})$$\n",
    "    The iterative process terminates when the predefined number of iterations has been reached or the error of the complete classifier $F(x)$ on the training set is 0.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Stump \n",
    "The base classifier used is decision stump, which is a single layered decision tree. It only looks at one feature in the data set. If the value is smaller(or lager) than the threshold, assign the object to one class. If the value is larger or equal to(smaller or equal to) the threshold, assign it to the other class. The **classify** function performs the classification of decision stump classifier. The information of the classifier is maintained in a dict, including the threshold, the feature to look at, and the range(smaller or larger than the threshold) which belongs to 1-class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def classify(dataSet, classifier):\n",
    "    threshold = classifier['threshold']\n",
    "    featureIndex = classifier['featureIndex']\n",
    "    numberOfRows = dataSet.shape[0]\n",
    "    labels = ones((numberOfRows,1))\n",
    "    if classifier['operator'] == 'lt':                   #'lt' means 'less than'\n",
    "        labels[dataSet[:,featureIndex]>=threshold] = -1\n",
    "    else:\n",
    "        labels[dataSet[:,featureIndex]<threshold] = -1\n",
    "    return labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer of Decision Stump\n",
    "The decision stump is trained by enumerating all possibilities, which contains the combination of three variables:\n",
    "1. features (correspond to **featureIndex** in the code)\n",
    "2. position the cut the range of the chosen feature (correspond to **minValue + i * stepSize** in the code)\n",
    "3. range that belongs to the 1-class(left or right of the cutting point)(correspond to **operator** in the code)\n",
    "\n",
    "So we have a nested loop with three layers. For the second variable, we cut the range of a feature $[minValue, maxValue]$ into $numberOfPieces$ pieces. We enumerate all the cutting points and in each iteration we use one cutting point as the point to separate this feature into two parts. \n",
    "\n",
    "For each combination of the three variables mentioned above, we generate a decision stump and measure the error on the **weighted training set**. The decision with the lowest error will be returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#trainer of decision stump\n",
    "def trainStump(dataSet, weights):\n",
    "    numberOfFeatures = dataSet.shape[1] - 1\n",
    "    numberOfRows = dataSet.shape[0]\n",
    "    numberOfPieces = 100\n",
    "    bestClassifier = {}\n",
    "    bestError = float(\"inf\")\n",
    "    #enumerate all possible features\n",
    "    for featureIndex in range(numberOfFeatures):\n",
    "        minValue=dataSet[:,featureIndex].min()\n",
    "        maxValue=dataSet[:,featureIndex].max()\n",
    "        \n",
    "        #cut the range of the feature into numberOfPieces pieces\n",
    "        stepSize = (maxValue - minValue)/numberOfPieces\n",
    "        #enumerate all possible cutting points\n",
    "        for i in range(-1,numberOfPieces+1):\n",
    "            #enumerate all possible operators \n",
    "            for operator in ['lt','gt']:\n",
    "                #generate the decision stump with the corresponding setting\n",
    "                classifier={}\n",
    "                classifier['threshold'] = minValue + i * stepSize\n",
    "                classifier['featureIndex'] = featureIndex\n",
    "                classifier['operator'] = operator\n",
    "                #calculated the error on the weighted training set\n",
    "                labels = classify(dataSet, classifier)\n",
    "                error = zeros((numberOfRows,1))\n",
    "                #perform element-wise multiply so that in the resulting column vector, misclassified data points correspond to -1\n",
    "                temp = dataSet[:,numberOfFeatures].reshape(numberOfRows,1)*labels #we must do a reshape here because dataSet[:,numberOfFeatures] gives an array. We need a column vector                \n",
    "                error[temp[:,0] == -1] = 1\n",
    "                #the final error is calculated with matrix multiplication\n",
    "                weightError = mat(weights).T*mat(error)\n",
    "                #return the decision stump with the lowest \n",
    "                if(weightError[0,0]<bestError):\n",
    "                    bestError = weightError[0,0]\n",
    "                    bestClassifier=classifier\n",
    "                    #the classification result of the best classifier on the training set\n",
    "                    bestLabels = labels\n",
    "    return bestClassifier , bestError, bestLabels\n",
    "\n",
    "#play with this classifier tainer. \n",
    "dataSet = df.values    \n",
    "numberOfRows = dataSet.shape[0]\n",
    "#assume that all the training points have equal weights\n",
    "weights = ones((numberOfRows,1))\n",
    "weights = weights/numberOfRows\n",
    "classifier,error,labels=trainStump(dataSet,weights)\n",
    "classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The complete Adaboost\n",
    "The body of adaboost algorithm is implemented in function **adaBoost**. Initially, all the training data points have the same weight. In each of the **numberOfIterations** iterations, first train a new decision stump on the weighted training set, then calculate the **alpha** for this classifier and the new weights of data points for the next iteration.\n",
    "\n",
    "The method **applyFullClassifier** apply the complete boosted classifier and return thepredictions and the error on the training set. The predictions of individual base classifiers are multiplied with weights of the classifier and summed up. The sign of the sum correspond to the label of the complete classifier.\n",
    "\n",
    "The method **applyFullClassifierWithoutError** apply the complete boosted classifier without returning error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#apply the complete boosted classifier and return thepredictions and the error on the training set\n",
    "def applyFullClassifier(dataSet,classifiers):\n",
    "    numberOfRows=dataSet.shape[0]\n",
    "    numberOfFeatures=dataSet.shape[1]-1\n",
    "    prediction=zeros((numberOfRows,1))\n",
    "    for c in classifiers.keys():\n",
    "        item = classifiers[c]\n",
    "        classifier = item['classifier']\n",
    "        alpha = item['alpha']\n",
    "        prediction = prediction + alpha*classify(dataSet, classifier)\n",
    "    prediction[prediction[:,0]<0]=-1\n",
    "    prediction[prediction[:,0]>=0]=1\n",
    "    error = zeros((numberOfRows,1))\n",
    "    temp = dataSet[:,numberOfFeatures].reshape(numberOfRows,1)*prediction\n",
    "    error[temp[:,0] == -1] = 1\n",
    "    return prediction, error\n",
    "\n",
    "#body of adaboost\n",
    "def adaBoost(dataSet, numberOfIterations):\n",
    "    classifiers = {}\n",
    "    numberOfRows = dataSet.shape[0]\n",
    "    numberOfFeatures=dataSet.shape[1] - 1\n",
    "    #weights of training data points, initially all data points have the same weight.\n",
    "    weights = ones((numberOfRows,1))\n",
    "    weights = weights/numberOfRows\n",
    "    labelsOfFullClassifier = zeros((numberOfRows,1))\n",
    "    expectedLabels=dataSet[:,numberOfFeatures].reshape(numberOfRows,1)\n",
    "    for i in range(numberOfIterations):\n",
    "        classifiers[i] = {}\n",
    "        #train a new classifier. Here error is P_m in the formula\n",
    "        bestClassifier , error, labels = trainStump(dataSet,weights)\n",
    "        classifiers[i]['classifier'] = bestClassifier\n",
    "        #update alpha according based on the error of the newly trained based classifier on the weighted training set.\n",
    "        alpha = 0.5*log((1-error)/error)\n",
    "        classifiers[i]['alpha'] = alpha\n",
    "        #update the weight of the training points \n",
    "        weights = weights * exp(expectedLabels * labels *(0-1)*alpha)\n",
    "        z = weights.sum()\n",
    "        weights =  weights / z\n",
    "        #apply the complete boosted classifier on the dataset\n",
    "        predictions, errors =applyFullClassifier(dataSet,classifiers)\n",
    "        numberOfMisclassifiedObjects = errors.sum()\n",
    "        #if no training points are misclassified, exit looping\n",
    "        print(str(numberOfMisclassifiedObjects)+\" objects wrongly classified\")\n",
    "        if numberOfMisclassifiedObjects == 0:\n",
    "            break\n",
    "    \n",
    "    return classifiers\n",
    "\n",
    "#apply the complete boosted classifier without returning error.\n",
    "def applyFullClassifierWithoutError(dataSet,classifiers):\n",
    "    numberOfRows=dataSet.shape[0]\n",
    "    numberOfFeatures=dataSet.shape[1]-1\n",
    "    prediction=zeros((numberOfRows,1))\n",
    "    for c in classifiers.keys():\n",
    "        item = classifiers[c]\n",
    "        classifier = item['classifier']\n",
    "        alpha = item['alpha']\n",
    "        prediction = prediction + alpha*classify(dataSet, classifier)\n",
    "    prediction[prediction[:,0]<0]=-1\n",
    "    prediction[prediction[:,0]>=0]=1\n",
    "    return prediction\n",
    "\n",
    "boostedClassifier = adaBoost(dataSet,10)\n",
    "boostedClassifier\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the decision boundary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "h=0.1\n",
    "x_min, x_max = dataSet[:, 0].min() - 1, dataSet[:, 0].max() + 1\n",
    "y_min, y_max = dataSet[:, 1].min() - 1, dataSet[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "# here \"model\" is your model's prediction (classification) function\n",
    "Z = applyFullClassifierWithoutError(np.c_[xx.ravel(), yy.ravel()],boostedClassifier) \n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "Z\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "plt.axis('off')\n",
    "\n",
    "# Plot also the training points\n",
    "plt.scatter(dataSet[:, 0], dataSet[:, 1], c=[cmap.get(c, 'black') for c in df.label], cmap=plt.cm.Paired)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
